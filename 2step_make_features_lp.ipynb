{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8c744b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-08-27T15:37:14.231017+03:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.4\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-46-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36701f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "notebookstart= time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382e3549",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms #import (Compose, Normalize, Resize, ToPILImage,\n",
    "                                   # ToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d749c5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from pillow_heif import register_heif_opener\n",
    "register_heif_opener() # for using Image.open for .heic without changes\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77083f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28aa7cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy      : 1.23.2\n",
      "cv2        : 4.6.0\n",
      "torch      : 1.12.1\n",
      "torchvision: 0.13.1\n",
      "PIL        : 9.2.0\n",
      "pandas     : 1.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b4e0e",
   "metadata": {},
   "source": [
    "Блок для воспроизводимости результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c1a88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed the RNG for all devices (both CPU and CUDA)\n",
    "#torch.manual_seed(1984)\n",
    "\n",
    "#Disabling the benchmarking feature causes cuDNN to deterministically select an algorithm, \n",
    "#possibly at the cost of reduced performance.\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# for custom operators,\n",
    "import random\n",
    "random.seed(5986721)\n",
    "\n",
    "# \n",
    "np.random.seed(62185)\n",
    "\n",
    "#sklearn take seed from a line abowe\n",
    "\n",
    "CB_RANDOMSEED  = 309487\n",
    "XGB_RANDOMSEED = 56\n",
    "LGB_RANDOMSEED = 874256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758092ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8260b1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIR_DATA = os.path.join(os.getcwd(), 'data')\n",
    "DIR_SUBM = os.path.join(os.getcwd(), 'subm')\n",
    "DIR_SUBM_TRAIN = os.path.join(os.getcwd(), 'subm', 'train')\n",
    "DIR_DATA_TRAIN = os.path.join(DIR_DATA, 'train')\n",
    "DIR_DATA_TEST  = os.path.join(DIR_DATA, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc6f0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f772da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95972a7f",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac1918c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DIR_DATA, 'train_upd.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DIR_DATA, 'test_upd.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe16c4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#115 img_1824.jpg - белая машина с белой рамкой"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3993bab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4204fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f3c3a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(outputchannels: Optional[int] = 1, aux_loss: Optional[bool] = False, freeze_backbone: Optional[bool] = False):\n",
    "    model = models.segmentation.deeplabv3_resnet101(\n",
    "        pretrained=True, progress=True)#, aux_loss=aux_loss)\n",
    "\n",
    "    if freeze_backbone is True:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    #model.classifier = models.segmentation.segmentation.DeepLabHead(\n",
    "    model.classifier = models.segmentation.deeplabv3.DeepLabHead(\n",
    "        2048, outputchannels)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaadaea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction pipeline\n",
    "def pred(inp_image: np.ndarray, inp_model):\n",
    "    preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                    ])\n",
    "\n",
    "    input_tensor = preprocess(inp_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = inp_model(input_batch)['out'][0]\n",
    "    \n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eddcc395",
   "metadata": {},
   "source": [
    "%%time\n",
    "#filename = os.path.join(DIR_DATA_TRAIN, tmp.image_name)\n",
    "# Loading an image\n",
    "#img = Image.open(f'{filename}').convert('RGB')\n",
    "\n",
    "\n",
    "\n",
    "tmp = train_df.loc[115, :]\n",
    "\n",
    "img = Image.open(os.path.join(DIR_DATA_TRAIN, tmp.image_name))\n",
    "img = np.array(img)\n",
    "sub_img = img[int(tmp.y_min) : int(tmp.y_max),\n",
    "              int(tmp.x_min) : int(tmp.x_max)\n",
    "             ]\n",
    "\n",
    "# Defining a threshold for predictions\n",
    "threshold = 0.1 # 0.1 seems appropriate for the pre-trained model\n",
    "\n",
    "# Predict\n",
    "#output = pred(img, model)\n",
    "output = pred(sub_img, model)\n",
    "\n",
    "output = (output > threshold).type(torch.IntTensor)\n",
    "output_np = output.cpu().numpy()[0]\n",
    "\n",
    "# Extracting coordinates\n",
    "result = np.where(output_np > 0)\n",
    "coords = list(zip(result[0], result[1]))\n",
    "\n",
    "\n",
    "x_min = sorted(coords, key = lambda x: x[0])[0][0]\n",
    "y_min = sorted(coords, key = lambda x: x[1])[0][1]\n",
    "x_max = sorted(coords, key = lambda x: x[0])[-1][0]\n",
    "y_max = sorted(coords, key = lambda x: x[1])[-1][1]\n",
    "\n",
    "# Overlay the original image\n",
    "#for cord in coords:\n",
    "    #frame.putpixel((cord[1], cord[0]), (255, 0, 0))\n",
    "    #img.putpixel((cord[1], cord[0]), (255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "573c8cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_plate_features_tuple(inp_row: str, inp_folder: str, inp_model) -> Tuple[int, int, int, int]:\n",
    "    \n",
    "    #print(inp_row)\n",
    "    #return 0\n",
    "\n",
    "    x_min = 0\n",
    "    y_min = 0\n",
    "    x_max = 0\n",
    "    y_max = 0\n",
    "    \n",
    "    # найдена licence plate\n",
    "    if inp_row.car_y_min > 0:\n",
    "\n",
    "        #img = Image.open(os.path.join(DIR_DATA_TRAIN, tmp.image_name))\n",
    "        img = Image.open(os.path.join(inp_folder, inp_row.image_name))\n",
    "        img = np.array(img)\n",
    "        sub_img = img[int(inp_row.car_y_min) : int(inp_row.car_y_max),\n",
    "                      int(inp_row.car_x_min) : int(inp_row.car_x_max)\n",
    "                     ]\n",
    "\n",
    "        # Defining a threshold for predictions\n",
    "        threshold = 0.1 # 0.1 seems appropriate for the pre-trained model\n",
    "\n",
    "        # Predict\n",
    "        output = pred(sub_img, inp_model)\n",
    "\n",
    "\n",
    "        output = (output > threshold).type(torch.IntTensor)\n",
    "        output_np = output.cpu().numpy()[0]\n",
    "\n",
    "        # Extracting coordinates\n",
    "        result = np.where(output_np > 0)\n",
    "        coords = list(zip(result[0], result[1]))\n",
    "\n",
    "        if len(coords) != 0:\n",
    "            x_min = sorted(coords, key = lambda x: x[0])[0][0]\n",
    "            y_min = sorted(coords, key = lambda x: x[1])[0][1]\n",
    "            x_max = sorted(coords, key = lambda x: x[0])[-1][0]\n",
    "            y_max = sorted(coords, key = lambda x: x[1])[-1][1]\n",
    "    \n",
    "    return (x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "158a6e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_plate_features(inp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    #inp_df.tmp = inp_df.tmp.apply(lambda x: (x))\n",
    "    \n",
    "    inp_df['plate_x_min'] = inp_df.tmp.apply(lambda x: float(x[0]))\n",
    "    inp_df['plate_y_min'] = inp_df.tmp.apply(lambda x: float(x[1]))\n",
    "    inp_df['plate_x_max'] = inp_df.tmp.apply(lambda x: float(x[2]))\n",
    "    inp_df['plate_y_max'] = inp_df.tmp.apply(lambda x: float(x[3]))\n",
    "    \n",
    "    inp_df['plate_w'] = inp_df.plate_x_max - inp_df.plate_x_min\n",
    "    inp_df['plate_h'] = inp_df.plate_y_max - inp_df.plate_y_min\n",
    "    \n",
    "    #inp_df.drop(['tmp'], axis = 0, inplace = True)\n",
    "    \n",
    "    return inp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b1be3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4fdb30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/dennisbappert/pytorch-licenseplate-segmentation\n",
    "# Load the model:\n",
    "model = create_model()\n",
    "checkpoint = torch.load('./models_weights/model_v2.pth', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "_ = model.eval()\n",
    "_ = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0265caf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before  (530, 16) (517, 15)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01752924919128418,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 530,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea9ae1461714c8bb889451538cc07a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011058330535888672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 517,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b162ccbdc551433396788f85e0d6740f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after   (530, 17) (517, 16)\n"
     ]
    }
   ],
   "source": [
    "print('before ', train_df.shape, test_df.shape)\n",
    "train_df['tmp'] = train_df.progress_apply(lambda x: get_plate_features_tuple(x, DIR_DATA_TRAIN, model), axis = 1)\n",
    "test_df['tmp'] = test_df.progress_apply(lambda x: get_plate_features_tuple(x, DIR_DATA_TEST, model), axis = 1)\n",
    "print('after  ', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc8d667f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before  (530, 17) (517, 16)\n",
      "after   (530, 23) (517, 22)\n"
     ]
    }
   ],
   "source": [
    "print('before ', train_df.shape, test_df.shape)\n",
    "train_df = get_plate_features(train_df)\n",
    "test_df = get_plate_features(test_df)\n",
    "print('after  ', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a9b01ce-f52e-4efc-85e7-750edf0b95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in ['plate_w', 'plate_h']:\n",
    "    train_df[f'log_{el}'] = train_df[el].apply(lambda x: np.log(x))\n",
    "    test_df[f'log_{el}'] = test_df[el].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b69dea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DIR_DATA, 'train_upd.csv'), index = False)\n",
    "test_df.to_csv(os.path.join(DIR_DATA,  'test_upd.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3d4cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "063dc202",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>car_x_min</th>\n",
       "      <th>car_y_min</th>\n",
       "      <th>car_x_max</th>\n",
       "      <th>car_y_max</th>\n",
       "      <th>car_conf</th>\n",
       "      <th>car_class</th>\n",
       "      <th>car_h</th>\n",
       "      <th>car_w</th>\n",
       "      <th>log_car_x_min</th>\n",
       "      <th>...</th>\n",
       "      <th>log_car_y_max</th>\n",
       "      <th>log_car_h</th>\n",
       "      <th>log_car_w</th>\n",
       "      <th>tmp</th>\n",
       "      <th>plate_x_min</th>\n",
       "      <th>plate_y_min</th>\n",
       "      <th>plate_x_max</th>\n",
       "      <th>plate_y_max</th>\n",
       "      <th>plate_w</th>\n",
       "      <th>plate_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_2019.jpg</td>\n",
       "      <td>1534.937988</td>\n",
       "      <td>1122.445801</td>\n",
       "      <td>2258.129395</td>\n",
       "      <td>1753.614258</td>\n",
       "      <td>0.922432</td>\n",
       "      <td>2.0</td>\n",
       "      <td>631.168457</td>\n",
       "      <td>723.191406</td>\n",
       "      <td>7.336245</td>\n",
       "      <td>...</td>\n",
       "      <td>7.469434</td>\n",
       "      <td>6.447573</td>\n",
       "      <td>6.583674</td>\n",
       "      <td>(349, 241, 419, 502)</td>\n",
       "      <td>349.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_2692.jpg</td>\n",
       "      <td>1778.514648</td>\n",
       "      <td>1269.690674</td>\n",
       "      <td>2090.799072</td>\n",
       "      <td>1552.986694</td>\n",
       "      <td>0.809172</td>\n",
       "      <td>2.0</td>\n",
       "      <td>283.296021</td>\n",
       "      <td>312.284424</td>\n",
       "      <td>7.483534</td>\n",
       "      <td>...</td>\n",
       "      <td>7.347935</td>\n",
       "      <td>5.646492</td>\n",
       "      <td>5.743914</td>\n",
       "      <td>(146, 97, 174, 211)</td>\n",
       "      <td>146.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_2417.jpg</td>\n",
       "      <td>1622.073120</td>\n",
       "      <td>1102.892822</td>\n",
       "      <td>2346.047363</td>\n",
       "      <td>1695.427490</td>\n",
       "      <td>0.922528</td>\n",
       "      <td>2.0</td>\n",
       "      <td>592.534668</td>\n",
       "      <td>723.974243</td>\n",
       "      <td>7.391460</td>\n",
       "      <td>...</td>\n",
       "      <td>7.435690</td>\n",
       "      <td>6.384409</td>\n",
       "      <td>6.584756</td>\n",
       "      <td>(51, 245, 396, 629)</td>\n",
       "      <td>51.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>629.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_2279.jpg</td>\n",
       "      <td>1669.779541</td>\n",
       "      <td>1172.031006</td>\n",
       "      <td>2143.335693</td>\n",
       "      <td>1597.427612</td>\n",
       "      <td>0.918139</td>\n",
       "      <td>2.0</td>\n",
       "      <td>425.396606</td>\n",
       "      <td>473.556152</td>\n",
       "      <td>7.420447</td>\n",
       "      <td>...</td>\n",
       "      <td>7.376150</td>\n",
       "      <td>6.053022</td>\n",
       "      <td>6.160270</td>\n",
       "      <td>(307, 153, 348, 321)</td>\n",
       "      <td>307.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_2701.jpg</td>\n",
       "      <td>1309.571289</td>\n",
       "      <td>1094.303101</td>\n",
       "      <td>2470.923096</td>\n",
       "      <td>1935.219482</td>\n",
       "      <td>0.925146</td>\n",
       "      <td>2.0</td>\n",
       "      <td>840.916382</td>\n",
       "      <td>1161.351807</td>\n",
       "      <td>7.177455</td>\n",
       "      <td>...</td>\n",
       "      <td>7.567976</td>\n",
       "      <td>6.734492</td>\n",
       "      <td>7.057340</td>\n",
       "      <td>(444, 392, 617, 1038)</td>\n",
       "      <td>444.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>617.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>646.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name    car_x_min    car_y_min    car_x_max    car_y_max  car_conf  \\\n",
       "0  img_2019.jpg  1534.937988  1122.445801  2258.129395  1753.614258  0.922432   \n",
       "1  img_2692.jpg  1778.514648  1269.690674  2090.799072  1552.986694  0.809172   \n",
       "2  img_2417.jpg  1622.073120  1102.892822  2346.047363  1695.427490  0.922528   \n",
       "3  img_2279.jpg  1669.779541  1172.031006  2143.335693  1597.427612  0.918139   \n",
       "4  img_2701.jpg  1309.571289  1094.303101  2470.923096  1935.219482  0.925146   \n",
       "\n",
       "   car_class       car_h        car_w  log_car_x_min  ...  log_car_y_max  \\\n",
       "0        2.0  631.168457   723.191406       7.336245  ...       7.469434   \n",
       "1        2.0  283.296021   312.284424       7.483534  ...       7.347935   \n",
       "2        2.0  592.534668   723.974243       7.391460  ...       7.435690   \n",
       "3        2.0  425.396606   473.556152       7.420447  ...       7.376150   \n",
       "4        2.0  840.916382  1161.351807       7.177455  ...       7.567976   \n",
       "\n",
       "   log_car_h  log_car_w                    tmp  plate_x_min plate_y_min  \\\n",
       "0   6.447573   6.583674   (349, 241, 419, 502)        349.0       241.0   \n",
       "1   5.646492   5.743914    (146, 97, 174, 211)        146.0        97.0   \n",
       "2   6.384409   6.584756    (51, 245, 396, 629)         51.0       245.0   \n",
       "3   6.053022   6.160270   (307, 153, 348, 321)        307.0       153.0   \n",
       "4   6.734492   7.057340  (444, 392, 617, 1038)        444.0       392.0   \n",
       "\n",
       "   plate_x_max  plate_y_max  plate_w  plate_h  \n",
       "0        419.0        502.0     70.0    261.0  \n",
       "1        174.0        211.0     28.0    114.0  \n",
       "2        396.0        629.0    345.0    384.0  \n",
       "3        348.0        321.0     41.0    168.0  \n",
       "4        617.0       1038.0    173.0    646.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf383bc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81ff83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9136f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca7638",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea305c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4056f7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3907d100",
   "metadata": {},
   "source": [
    "def get_aspect_ratio(inp_points: np.ndarray) -> float:\n",
    "    \n",
    "    upper_len  = inp_points[1, 0, 0] - inp_points[0, 0, 0] \n",
    "    bottom_len = inp_points[2, 0, 0] - inp_points[3, 0, 0] \n",
    "    aver_len   = (upper_len + bottom_len)/2\n",
    "    \n",
    "    upper_hi  = inp_points[3, 0, 1] - inp_points[0, 0, 1] \n",
    "    bottom_hi = inp_points[2, 0, 1] - inp_points[1, 0, 1] \n",
    "    aver_hi   = (upper_hi + bottom_hi)/2\n",
    "    \n",
    "    return aver_hi / aver_len"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58d02990",
   "metadata": {},
   "source": [
    "# https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv\n",
    "def apply_brightness_contrast(input_img: np.ndarray, brightness: Optional[int] = 0, contrast: Optional[int] = 0) -> np.ndarray:\n",
    "    \n",
    "    if brightness != 0:\n",
    "        if brightness > 0:\n",
    "            shadow = brightness\n",
    "            highlight = 255\n",
    "        else:\n",
    "            shadow = 0\n",
    "            highlight = 255 + brightness\n",
    "        alpha_b = (highlight - shadow)/255\n",
    "        gamma_b = shadow\n",
    "        \n",
    "        buf = cv2.addWeighted(input_img, alpha_b, input_img, 0, gamma_b)\n",
    "    else:\n",
    "        buf = input_img.copy()\n",
    "    \n",
    "    if contrast != 0:\n",
    "        f = 131*(contrast + 127)/(127*(131-contrast))\n",
    "        alpha_c = f\n",
    "        gamma_c = 127*(1-f)\n",
    "        \n",
    "        buf = cv2.addWeighted(buf, alpha_c, buf, 0, gamma_c)\n",
    "\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a45326f1",
   "metadata": {},
   "source": [
    "tmp = train_df.loc[115, :]\n",
    "\n",
    "img = Image.open(os.path.join(DIR_DATA_TRAIN, tmp.image_name))\n",
    "img = np.array(img)\n",
    "\n",
    "sub_img = img[int(tmp.y_min) : int(tmp.y_max),\n",
    "              int(tmp.x_min) : int(tmp.x_max)\n",
    "             ]\n",
    "ttl_img = sub_img.copy()\n",
    "\n",
    "\n",
    "# ------------ контраст\n",
    "contrasted_img = cv2.cvtColor(sub_img, cv2.COLOR_BGR2LAB)\n",
    "l_channel, a, b = cv2.split(contrasted_img)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "cl = clahe.apply(l_channel)\n",
    "# merge the CLAHE enhanced L-channel with the a and b channel\n",
    "limg = cv2.merge((cl,a,b))\n",
    "\n",
    "# Converting image from LAB Color model to BGR color spcae\n",
    "contrasted_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "\n",
    "#alpha = 1.5 # Contrast control (1.0-3.0)\n",
    "#beta = 0 # Brightness control (0-100)\n",
    "#contrasted_img = cv2.convertScaleAbs(sub_img, alpha=alpha, beta=beta)\n",
    "\n",
    "\n",
    "ttl_img = np.concatenate([ttl_img, contrasted_img], axis = 1)\n",
    "\n",
    "\n",
    "# ------------ грани\n",
    "#gray_image = cv2.bilateralFilter(gray_image, 11, 17, 17) \n",
    "#gray_image = cv2.bilateralFilter(sub_img, 11, 17, 17) \n",
    "gray_image = cv2.bilateralFilter(contrasted_img, 11, 17, 17) \n",
    "\n",
    "edged = cv2.Canny(gray_image, 100, 200) \n",
    "#edged = cv2.Canny(gray_image, 5, 200) \n",
    "edged2 = cv2.cvtColor(edged, cv2.COLOR_GRAY2RGB)\n",
    "ttl_img = np.concatenate([ttl_img, edged2], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# ------------ контуры\n",
    "cnts,new = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "sub_img1 = sub_img.copy()\n",
    "cv2.drawContours(sub_img1, cnts, -1, (0, 255, 0), 3)\n",
    "ttl_img = np.concatenate([ttl_img, sub_img1], axis = 1)\n",
    "#ttl_img2 = sub_img1.copy()\n",
    "\n",
    "\n",
    "# ------------ 10 крупнейших контуров\n",
    "cnts = sorted(cnts, key = cv2.contourArea, reverse = True) [:10]\n",
    "sub_img2 = sub_img.copy()\n",
    "cv2.drawContours(sub_img2, cnts, -1, (0, 255, 0) ,3)\n",
    "#ttl_img2 = np.concatenate([ttl_img2, sub_img2], axis = 1)\n",
    "ttl_img2 = sub_img2.copy()\n",
    "\n",
    "\n",
    "# ------------ вычленение номера и региона среди контуров\n",
    "lp_region   = np.zeros((4, 1, 2), dtype = np.int32)\n",
    "legal_plate = np.zeros((4, 1, 2), dtype = np.int32)\n",
    "sub_img4 = sub_img.copy()\n",
    "approx_list = []\n",
    "for idx, c in enumerate(cnts):\n",
    "    \n",
    "    perimeter = cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, 0.005 * perimeter, True) #0.018\n",
    "    approx_list.append(approx)\n",
    "    \n",
    "    cv2.drawContours(sub_img4, [approx], -1, (0, 255, 0) ,3)\n",
    "    #if len(approx) == 4:\n",
    "    #    ar = get_aspect_ratio(approx) \n",
    "    #    if ar < 0.8 and ar > 0.65:\n",
    "    #        print('region ', idx, ar)\n",
    "    #        lp_region = approx\n",
    "    #        \n",
    "    #    if ar < 0.25 and ar > 0.15:\n",
    "    #        print('legal plate ', idx, ar)\n",
    "    #        legal_plate = approx\n",
    "\n",
    "\n",
    "sub_img5 = sub_img.copy()\n",
    "for idx, el in enumerate(approx_list):\n",
    "    \n",
    "    perimeter = cv2.arcLength(el, True)\n",
    "    approx = cv2.approxPolyDP(el, 0.022 * perimeter, True) #0.018\n",
    "    \n",
    "    cv2.drawContours(sub_img5, [approx], -1, (0, 255, 0) ,3)\n",
    "    print(idx, len(approx))\n",
    "    if len(approx) == 4:\n",
    "        ar = get_aspect_ratio(approx) \n",
    "        print(idx, len(approx), ar)\n",
    "        if ar < 0.86 and ar > 0.65:\n",
    "            print('region ', idx, ar)\n",
    "            lp_region = approx\n",
    "            \n",
    "        if ar < 0.26 and ar > 0.15:\n",
    "            print('legal plate ', idx, ar)\n",
    "            legal_plate = approx\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "ttl_img2 = np.concatenate([ttl_img2, sub_img4], axis = 1)\n",
    "ttl_img2 = np.concatenate([ttl_img2, sub_img5], axis = 1)\n",
    "            \n",
    "sub_img3 = sub_img.copy()\n",
    "cv2.drawContours(sub_img3, [legal_plate], -1, (0, 255, 0) ,3)\n",
    "if lp_region[0, 0, 0] != 0:\n",
    "    cv2.drawContours(sub_img3, [lp_region], -1, (0, 0, 255), 3)\n",
    "ttl_img2 = np.concatenate([ttl_img2, sub_img3], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "ttl_img = np.concatenate([ttl_img, ttl_img2])\n",
    "\n",
    "ttl_img = cv2.resize(ttl_img, [1780, 814])\n",
    "\n",
    "cv2.imshow(\"Top 10 contours\", ttl_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0aea59bb",
   "metadata": {},
   "source": [
    "tmp = train_df.loc[115, :]\n",
    "\n",
    "img = Image.open(os.path.join(DIR_DATA_TRAIN, tmp.image_name))\n",
    "img = np.array(img)\n",
    "\n",
    "sub_img = img[int(tmp.y_min) : int(tmp.y_max),\n",
    "              int(tmp.x_min) : int(tmp.x_max)\n",
    "             ]\n",
    "\n",
    "# add contrast\n",
    "#contrasted_img = cv2.cvtColor(sub_img, cv2.COLOR_BGR2LAB)\n",
    "#l_channel, a, b = cv2.split(contrasted_img)\n",
    "\n",
    "#clahe = cv2.createCLAHE(clipLimit=6.0, tileGridSize=(4,4))\n",
    "#cl = clahe.apply(l_channel)\n",
    "# merge the CLAHE enhanced L-channel with the a and b channel\n",
    "#limg = cv2.merge((cl,a,b))\n",
    "\n",
    "\n",
    "# Converting image from LAB Color model to BGR color spcae\n",
    "#contrasted_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 1.5 # Contrast control (1.0-3.0)\n",
    "beta = 0 # Brightness control (0-100)\n",
    "\n",
    "contrasted_img = cv2.convertScaleAbs(sub_img, alpha=alpha, beta=beta)\n",
    "\n",
    "\n",
    "\n",
    "ttl_img = np.concatenate([sub_img, contrasted_img], axis = 1)\n",
    "\n",
    "cv2.imshow(\"original image\", ttl_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2b63e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardist22",
   "language": "python",
   "name": "cardist22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
